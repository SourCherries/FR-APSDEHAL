{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FR_APSDEHAL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQ+K7Yyv92g2CtgIGi4/YO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SourCherries/FR-APSDEHAL/blob/master/FR_APSDEHAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Face recognition CNN by APSDEHAL from"
      ],
      "metadata": {
        "id": "PxBsVKqgmnbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimal script to run forward pass on model using initial parameters. Uncertain whether initial paramters are random. Goal is to pinpoint source of error when running main.py where weights appear to be on cpu despite putting entire model onto gpu."
      ],
      "metadata": {
        "id": "dkOCYletrwQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on [pytorch example](https://pytorch.org/hub/pytorch_vision_vgg/) and on [CNN by APSDEHAL](https://github.com/apsdehal/Face-Recognition)."
      ],
      "metadata": {
        "id": "KmfTccA7rzoF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9TP-1-M9me_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dde1cf7-7bc2-4c29-f2a0-7e133b65c1ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FR-APSDEHAL'...\n",
            "remote: Enumerating objects: 89, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 89 (delta 12), reused 0 (delta 0), pack-reused 64\u001b[K\n",
            "Unpacking objects: 100% (89/89), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/SourCherries/FR-APSDEHAL.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(1, '/content/FR-APSDEHAL')\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from model_noargs import Network\n",
        "from constants import IMG_SIZE, NUM_CHANNELS\n",
        "\n",
        "from torchvision.io import read_image"
      ],
      "metadata": {
        "id": "G7UHkGnsr4tw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load image(s)"
      ],
      "metadata": {
        "id": "DkL7MZFN8YoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "judi = read_image(\"/content/FR-APSDEHAL/judi-dench-256.png\")\n",
        "kathy = read_image(\"/content/FR-APSDEHAL/kathleen-turner-256.png\")\n",
        "input_batch = torch.cat([torch.unsqueeze(judi, 0), torch.unsqueeze(kathy, 0)], dim=0)\n",
        "\n",
        "print(input_batch.shape)\n",
        "print(input_batch.dtype)"
      ],
      "metadata": {
        "id": "78moiD6J8bVK",
        "outputId": "3579b706-40ae-489d-ece1-951a4d95054f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 256, 256])\n",
            "torch.uint8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "input_image = Image.open(\"/content/FR-APSDEHAL/judi-dench-256.png\")\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "print(input_batch.shape)\n",
        "print(input_batch.dtype)\n",
        "print(input_batch.min())\n",
        "print(input_batch.max())"
      ],
      "metadata": {
        "id": "052El6qB_CzR",
        "outputId": "aaf89a2d-b257-401e-9137-457f410e4d7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 256, 256])\n",
            "torch.float32\n",
            "tensor(-2.1179)\n",
            "tensor(2.4286)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test model"
      ],
      "metadata": {
        "id": "oCGLFp6P8cVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Network()\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)"
      ],
      "metadata": {
        "id": "XJzKJe3i6JIU",
        "outputId": "c2391e61-54db-49b8-9215-8d0163192d80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/FR-APSDEHAL/model_noargs.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return nn.functional.log_softmax(self.fully_connected2(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.shape)\n",
        "print(output.dtype)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "i-NoaITVDi_V",
        "outputId": "5809346f-9a43-4b78-e0bd-3abdc6672116",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 15])\n",
            "torch.float32\n",
            "tensor([[-2.7510, -2.7007, -2.7054, -2.6757, -2.6779, -2.7398, -2.7487, -2.7057,\n",
            "         -2.7231, -2.7803, -2.7318, -2.6871, -2.6362, -2.6597, -2.7079]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "print(probabilities)"
      ],
      "metadata": {
        "id": "zMnPlvpGDxKn",
        "outputId": "1dd88b18-d132-402d-9d22-cf1b6a99b202",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0639, 0.0672, 0.0668, 0.0689, 0.0687, 0.0646, 0.0640, 0.0668, 0.0657,\n",
            "        0.0620, 0.0651, 0.0681, 0.0716, 0.0700, 0.0667])\n"
          ]
        }
      ]
    }
  ]
}